<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Identify and mitigate the distributional shift of proprioception observation in robotic imitation learning.">
  <meta name="keywords" content="Imitation Learning, Proprioception States, Distributional Shift">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://coffeepot1206.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://proprioception-shift.github.io">
            Proprioception Shift
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://coffeepot1206.github.io">Fuhang Kuang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=FiP-TVUAAAAJ">Jiacheng You</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yingdong-hu.github.io/">Yingdong Hu</a><sup>1,2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://tongzhangthu.github.io/">Tong Zhang</a><sup>1,2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://alvinwen428.github.io/">Chuan Wen</a><sup>1,2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://yang-gao.weebly.com/">Yang Gao</a><sup>1,2,3</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Qi Zhi Institute,</span>
            <span class="author-block"><sup>3</sup>Shanghai Artificial Intelligence Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                  </a>
              </span>
              <!-- Dataset Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body" style="text-align: center;">
      <img src="./static/images/avg_result.png" alt="Description of the image" style="width:1000px; height:auto;">
      <h2 class="subtitle has-text" style="width: 72%; margin: 0 auto; text-align: left;">
      </br>
      <b>Left:</b> Including proprioception observation (BC-Full) in imitation learning can result in sever performance drop compared to RGB-only observation (BC-RGB). Our method NADA can bring better performance with proprioception observation.
      <b>Right:</b> By measuring the distributional shift along time, we find that including proprioception observation will result in higher shift in proprioception, while our method NADA mitigates such shift.
      <!-- <b>Bottom Right:</b> Tasks of the 3 simulation benchmarks. -->
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="text-align: center;">Overview Video</h2>
    <br/>
    <div class="columns is-centered">
      <div class="column">
        <div class="content video-container">
          <video id="dollyzoom" controls playsinline controlsList="nodownload" height="100%">
            <source src="./static/videos/teaser.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3" style="text-align: center;">Real-Robot Results</h2>
    <!-- <br/>
    <h2 class="title is-4">Long-horizon Tasks</h2> -->
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h3 class="title is-5">Pick and Place</h3>
          <!-- <video id="dollyzoom" autoplay muted loop playsinline height="100%"> -->
          <video id="dollyzoom" controls muted autoplay loop playsinline controlsList="nodownload" height="100%">
            <source src="./static/videos/pick-place-demo.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h3 class="title is-5">Open Locker</h3>
        <div class="columns is-centered">
          <div class="column content">
            <!-- <video id="dollyzoom" autoplay muted loop playsinline height="100%"> -->
            <video id="dollyzoom" controls muted autoplay loop playsinline controlsList="nodownload" height="100%">
              <source src="./static/videos/open-locker-demo.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Matting. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Imitation learning models for robotic tasks typically rely on multi-modal inputs, such as RGB images, language, and proprioceptive states. While proprioception is intuitively important for decision-making and obstacle avoidance, simply incorporating all proprioceptive states leads to a surprising degradation in imitation learning performance.  In this work, we identify the underlying issue as the proprioception shift problem, where the distributions of proprioceptive states diverge significantly between training and deployment. 
          </p>
          <p>
            To address this challenge, we propose a domain adaptation framework that bridges the gap by utilizing rollout data collected during deployment. Using Wasserstein distance, we quantify the discrepancy between expert and rollout proprioceptive states and minimize this gap by adding noise to both sets of states, proportional to the Wasserstein distance. This strategy enhances robustness against proprioception shifts by aligning the training and deployment distributions.
          </p>
          <p>
            Experiments on robotic manipulation tasks demonstrate the efficacy of our method, enabling the imitation policy to leverage proprioception while mitigating its adverse effects. Our approach outperforms the naive solution which discards proprioception, and other baselines designed to address distributional shifts.	
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="dperact">Noise-Augmented Distribution Alignment</span></h2>

        <img src="./static/images/teaser.png" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
        </br>
          <p>
            <b>Strategies for incorporating proprioceptive observations (\(o_p\)) in robotic imitation learning.</b> (a) NADA (Ours): A two-pass approach where proprioception shift (\(W_T\)) between initial training/rollout data guides optimized noise (\(\sigma^*\)) injection into the training set for robust policy learning. Compared Baselines: (b) Full Observations: Using all sensory inputs directly. (c) Pure RGB: Discarding proprioception entirely. (d) Dropout Proprioception: Randomly zeroing out proprioceptive inputs during training. Our method provides a systematic framework that not only mitigates the proprioception shift but also preserves valuable information in proprioceptive observations.
          </p>

      </div>
    </div>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{
  }</code></pre>
  </div>
</section> -->
  
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://github.com/sgrv2-robot/sgrv2-robot.github.io">SGRv2</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
